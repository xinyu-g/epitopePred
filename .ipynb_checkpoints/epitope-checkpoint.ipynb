{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tdc.single_pred import Epitope\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
    "from sklearn.metrics import roc_auc_score, f1_score, average_precision_score, precision_score, recall_score, accuracy_score\n",
    "from copy import deepcopy\n",
    "torch.manual_seed(1)\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from sklearn.metrics import roc_auc_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "100%|██████████| 2.18M/2.18M [00:00<00:00, 7.34MiB/s]\n",
      "Loading...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data =  Epitope(name = 'IEDB_Jespersen')\n",
    "split = data.get_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = split['train']\n",
    "valid_data = split['valid']\n",
    "test_data = split['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 'Antigen'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2vocab(data):\n",
    "\tlength = len(data)\n",
    "\tvocab_set = set()\n",
    "\ttotal_length, positive_num = 0, 0\n",
    "\tfor i in range(length):\n",
    "\t\tantigen = data[X][i]\n",
    "\t\tvocab_set = vocab_set.union(set(antigen))\n",
    "\t\tY = data['Y'][i]\n",
    "\t\tassert len(antigen) > max(Y)\n",
    "\t\ttotal_length += len(antigen)\n",
    "\t\tpositive_num += len(Y)\n",
    "\treturn vocab_set, positive_num / total_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab, train_positive_ratio = data2vocab(train_data)\n",
    "valid_vocab, valid_positive_ratio = data2vocab(valid_data)\n",
    "test_vocab, test_positive_ratio = data2vocab(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_set = train_vocab.union(valid_vocab)\n",
    "vocab_set = vocab_set.union(test_vocab)\n",
    "vocab_lst = list(vocab_set)\n",
    "# logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(idx, length):\n",
    "\tlst = [0 for i in range(length)]\n",
    "\tlst[idx] = 1\n",
    "\treturn lst \n",
    "\n",
    "def zerohot(length):\n",
    "\treturn [0 for i in range(length)]\n",
    "\n",
    "# what is the maxlength here\n",
    "def standardize_data(data, vocab_lst, maxlength = 300):\n",
    "\tlength = len(data)\n",
    "\tstandard_data = []\n",
    "\tfor i in range(length):\n",
    "\t\tantigen = data[X][i]\n",
    "\t\tY = data['Y'][i] \n",
    "\t\tsequence = [onehot(vocab_lst.index(s), len(vocab_lst)) for s in antigen] \n",
    "\t\tlabels = [0 for i in range(len(antigen))]\n",
    "\t\tmask = [True for i in range(len(labels))] # labels and mask have the same length\n",
    "\t\tsequence += (maxlength-len(sequence)) * [zerohot(len(vocab_lst))] #pad to consistent length\n",
    "\t\tlabels += (maxlength-len(labels)) * [0] \n",
    "\t\tmask += (maxlength-len(mask)) * [False] # pad to maxlength\n",
    "\t\tfor y in Y:\n",
    "\t\t\tlabels[y] = 1 \t\t\n",
    "\t\tsequence, labels, mask = sequence[:maxlength], labels[:maxlength], mask[:maxlength]\n",
    "\t\tsequence, labels, mask = torch.FloatTensor(sequence), torch.FloatTensor(labels), torch.BoolTensor(mask) \n",
    "\t\t# print(sequence.shape, labels.shape, mask.shape)\n",
    "        # sequence is 2D, labels and mask are 1D\n",
    "\t\tstandard_data.append((sequence, labels, mask))\n",
    "\treturn standard_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_stand = standardize_data(train_data, vocab_lst)\n",
    "valid_data_stand = standardize_data(valid_data, vocab_lst)\n",
    "test_data_stand = standardize_data(test_data, vocab_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "\tdef __init__(self, data):\n",
    "\t\tself.sequences = [i[0] for i in data]\n",
    "\t\tself.labels = [i[1] for i in data]\n",
    "\t\tself.mask = [i[2] for i in data] \n",
    "\n",
    "\tdef __getitem__(self, index):\n",
    "\t\treturn self.sequences[index], self.labels[index], self.mask[index]\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = dataset(train_data_stand)\n",
    "valid_set = dataset(valid_data_stand)\n",
    "test_set = dataset(test_data_stand)\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, name, hidden_size, input_size, num_layers = 2):\n",
    "        super(RNN, self).__init__()\n",
    "        self.name = name \n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size \n",
    "        self.rnn = nn.LSTM(         # if use nn.RNN(), it hardly learns\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,         # rnn hidden unit\n",
    "            num_layers=num_layers,           # number of rnn layer\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, 1)\n",
    "        criterion = torch.nn.BCEWithLogitsLoss()  \n",
    "        self.opt = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape (batch, time_step, input_size)\n",
    "        # r_out shape (batch, time_step, output_size)\n",
    "        # h_n shape (n_layers, batch, hidden_size)\n",
    "        # h_c shape (n_layers, batch, hidden_size)\n",
    "        r_out, (h_n, h_c) = self.rnn(x, None)   # None represents zero initial hidden state\n",
    "\n",
    "        # choose r_out at the last time step\n",
    "        out = self.out(r_out)\n",
    "        out = out.squeeze(-1)\n",
    "        return out\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
